# L1&L2正则化

# 为什么L1稀疏，L2平滑？

## **角度一：梯度**

这个角度从权值的更新公式来看权值的收敛结果。

首先来看看L1和L2的梯度(导数的反方向）：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/L1&L2正则化/Untitled.png?raw=true)

所以不失一般性，我们假定：$w_i$等于不为0的正浮点数，学习速率$\lambda$为0.5)：

- L1的权值更新公式为$w_i=w_i-\lambda * 1$，也就是说权值每次更新都固定减少一个特定的值，那么经过若干次迭代之后，权值就有可能减少到0。
- L2的权值更新公式为$w_i = w_i-\lambda w_i$，也就是说权值每次都等于上一次的1/2，那么，虽然权值不断变小，但是因为每次都等于上一次的一半，所以很快会收敛到较小的值但不为0。

下面的图很直观的说明了这个变化趋势：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/L1&L2正则化/Untitled%201.png?raw=true)

## **角度二：优化**

正则化公式为：

$$
\begin{array}{l}\min _{w} L_{1}(w)=\min _{w} f(w)+\frac{\lambda}{n} \sum_{i=1}^{n}\left|w_{i}\right| \\\min _{w} L_{2}(w)=\min _{w} f(w)+\frac{\lambda}{2 n} \sum_{i=1}^{n} w_{i}^{2}\end{array}
$$

****从优化问题的视角来看****

$$
\begin{array}{c}\min _{x} L(w)<=>\min _{w} f(w) \\\text { s.t. } \sum_{i=1}^{n}\left|w_{i}\right|<C\end{array}
$$

L1正则的限制条件，**在坐标轴上显示则是一个正方形**，与坐标轴的交点分别是(0,C),(C,0),(0,-C),(-C,0)

L2正则的限制条件，**在坐标轴上显示则是一个圆，**与坐标轴的交点分别是(0,C),(C,0),(0,-C),(-C,0)

**图中蓝色和绿色圈表示经验风险等高线，即同一圈上的所有点，经验风险相等。**

![https://pic3.zhimg.com/80/v2-1f1ec507f78843e2b7aafb2b9daa88d5_720w.jpg?source=1940ef5c](https://pic3.zhimg.com/80/v2-1f1ec507f78843e2b7aafb2b9daa88d5_720w.jpg?source=1940ef5c)

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/L1&L2正则化/Untitled%202.png?raw=true)

## 角度三：****概率****

**为$f(w)$加入正则化，相当于参数$w$加先验，要求$w$满足某一分布。**

- **L1正则化相当于为w加入Laplace分布的先验，**
- **L2正则化相当于为w加入Gaussian分布的先验**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/L1&L2正则化/Untitled%203.png?raw=true)

- 在两边紫色部分，$P_G(w)<P_L(w)$说明Gauss分布中，值大的w更少，因此L2比L1更smooth。
- 在中间红色线条区域$P_G(w)<P_L(w)$，Gauss分布中，**值很小的w和值为0的w概率接近**。而laplace分布，值很小的w概率小于值为0的w。说明**高斯分布要求w小就行**，而Laplace分布要求w更多为0。因此L1比L2更Sparse.

❓ ****L1稀疏，L2平滑****

- **L1能产生等于0的权值，即能够剔除某些特征在模型中的作用（特征选择），产生稀疏的效果。**
- **L2可以得迅速得到比较小的权值，但是难以收敛到0，所以产生的不是稀疏而是平滑的效果。**

❓ L2正则化和过拟合

拟合过程中通常都倾向于让权值尽可能小，一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，**若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响;但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，**专业一点的说法是**抗扰动能力强**

