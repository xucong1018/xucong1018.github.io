# LSTM

## LSTM

[详解LSTM](https://zhuanlan.zhihu.com/p/42717426)

**LSTM通常用来处理时序问题，LSTM引入了门（gate）机制用于控制过去时间片特征的传递，能够解决RNN的长期依赖问题。**

### **遗忘门：控制上一细胞状态的遗忘**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled.png?raw=true)

 $sigmod$的输出是一个介于$[0,1]$内的值，一个训  练好的LSTM时，门的值绝大多数都非常接近0或者1。

### 输入**门:控制当前细胞状态的更新**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%201.png?raw=true)

 $\widetilde{C}_t$表示单元状态更新值，$i_t$为更新门。对细胞状态$C_t$进行更新:

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%202.png?raw=true)

### **输出门:控制当前时间隐藏状态的输出**

胞状态$C_t$已经被更新了，这里的$O_t$还是用了一个sigmoid函数

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%203.png?raw=true)

## **长期依赖(Long Term Dependencies)**

[RNN的梯度消失](https://zhuanlan.zhihu.com/p/105999316)

💡 **RNN 所谓梯度消失的真正含义是,梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。RNN中的总的梯度不会消失。**


**RNN可以表示为：**

$$
h_t = f(x_t,h_{t-1},\theta)
$$

 $h_t$是$t$时刻的隐藏状态，它与前一时刻的状态和当前时刻的输入有关系，$\theta$为可训练参数；

**模型利用梯度下降优化时有：**

$$
\frac{\partial h_t}{\partial \theta} = \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partial h_t}{\partial \theta}
$$

当前时刻的梯度是前一时刻梯度与当前运算梯度的函数；

> **梯度消失**
> 

根据梯度讨论长期依赖问题，公式存在连乘项，计算单项：

$$
\frac{\partial h_t}{\partial h_{t-1}} = f'*\theta
$$

连乘项可能趋近0，导致长距离梯度消失。

## RNN

RNN之所以在时序数据上有着优异的表现**是因为RNN将之前时间片的信息也用于计算当前时间片表达**。

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%204.png?raw=true)

RNN的数学表达式可以表示为

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%205.png?raw=true)

而传统的DNN的隐节点表示为

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%206.png?raw=true)

## GRU

GRU中只有两个门：分别是更新门和重置门。重置⻔控制了**上⼀时间步的隐藏状态如何流⼊当前时间步的候选状态**$\tilde{h}_t$，更新门**控制前一时间步和当前时间步的信息有多少是需要继续传递。**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/LSTM/Untitled%207.png?raw=true)

$$
\begin{aligned}
z_{t} &=\sigma\left(W_{z} \cdot\left[h_{t-1}, x_{t}\right]\right) \\
r_{t} &=\sigma\left(W_{r} \cdot\left[h_{t-1}, x_{t}\right]\right) \\
\tilde{h}_{t} &=\tanh \left(W \cdot\left[r_{t} * h_{t-1}, x_{t}\right]\right) \\
h_{t} &=\left(1-z_{t}\right) * h_{t-1}+z_{t} * \tilde{h}_{t}
\end{aligned}
$$

这里的遗忘$1-Z$和选择$Z$是联动的。对于**传递进来的维度信息，遗忘了多少权重 ，就会使用包含当前状态$\tilde{h}_t$所对应的权重进行弥补** 。以保持一种**”恒定“**状态。

💡 **LSTM、GRU的区别和适用场景？**

- GRU把**输入门和遗忘门**合成了一个单一的**更新门**
- 取消进行记忆单元（memory cell），而是直接在隐藏单元中利用门控直接进行更新
- LSTM相比于GRU参数更多，计算量更大。如果使用的场景对计算量和参数量有很大要求，那可能只能是GRU
