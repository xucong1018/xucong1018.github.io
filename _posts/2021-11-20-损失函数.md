# 损失函数

# **交叉熵 & MSE**

[](https://zhongqiang.blog.csdn.net/article/details/115603924?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2-115603924-blog-123700552.pc_relevant_multi_platform_whitelistv1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2-115603924-blog-123700552.pc_relevant_multi_platform_whitelistv1&utm_relevant_index=5)

[为什么均方差（MSE）不适合分类问题？交叉熵（cross-entropy）不适合回归问题？](https://www.cnblogs.com/USTC-ZCC/p/13219281.html)

### 1、信息量

$$
I(x)=-log(p(x))
$$

> *事件发生的概率越小，不确定性越大，相应的信息量越大*
> 

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/损失函数/Untitled.png?raw=true)

### 2、信息熵(香农)

- **信息熵就是发生一个事件我们得到的平均信息量大小。即信息熵其实是信息量的期望**。
- **物理中熵**表达系统混乱程度的状态量，信息熵表达信息的不确定性。

$$
H(X)=-\sum_{i=1}^nP(x_i)log(p(x_i))\quad(X=x_1;x_2;\cdots;x_n)
$$

### 3、**条件熵**

条件熵$H(Y \mid X)$ 表示在**已知随机变量X的条件下，随机变量Y的不确定性。**

$$
\begin{aligned}H(Y \mid X) &=\sum_{x \in X} p(x) H(Y \mid X=x) \\&=-\sum_{x \in X} p(x) \sum_{y \in Y} p(y \mid x) \log p(y \mid x) \\&=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y \mid x)\end{aligned}
$$

### 4、互信息

- **表示在确定随机变量X的情况下，对随机变量Y的不确定性减少的程度。**
- **互信息和信息增益的公式很像**，互信息中**两个随机变量的地位相同**；信息增益是**把一个变量看成减小另一个变量不确定度的手段**。二者的数值是相等。

$$
I(X ; Y)=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}=H(X)-H(X \mid Y)=H(Y)-H(Y \mid X)
$$

### 5、KL散度（相对熵）

**用于衡量两个概率分布的差异**

$$
\begin{aligned}D_{K L}(p \| q)&=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right)\\&=\sum_{i=1} p\left(x_{i}\right) \log \left(p\left(x_{i}\right)\right)-\sum_{i=1} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)\end{aligned}
$$

> **KL散度 = 交叉熵 - 真实分布的信息熵**
> 

### 6**、JS散度**

度量了**两个概率分布的相似度**，基于KL散度的变体，解决了**KL散度非对称**的问题。一般地，JS散度是对称的，其取值是0到1之间。

$$
J S(p \| q)=\frac{1}{2} K L\left(p \| \frac{p+q}{2}\right)+\frac{1}{2} K L\left(q \| \frac{p+q}{2}\right)
$$

### 7、交叉熵

交叉熵用来评估训练得到的**概率分布与真实分布的差异**。

真正能衡量概率分布之间差异的是相对熵。

$$
H(p,q)=\sum_{i=1} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)
$$

❓ **交叉熵与MSE区别**（**为什么分类问题用 CE，回归问题用MSE？）**

### 分类**不用MSE**

- **MSE权重更新较慢**
    
    sigmod函数：$\sigma(z)=\frac{1}{1+e^{-z}}$，导数：$\sigma(z)'=\sigma(z)(1-\sigma(z))$
    
    令$x$为输入，$y$为标签，$\hat{y}$为预测值
    
    - **MSE损失：$L=(y-\hat{y})^2/2$，其中$\hat{y}=\sigma(z),z=wx+b$,**  $**w$梯度计算**
    
    $$
    \begin{aligned}\frac{\partial L}{\partial w}&=\frac{\partial L}{\partial \hat{y}}*\sigma(z)'*x&=(\sigma(z)−y)\sigma(z)'x\end{aligned}
    $$
    
    > **其中包含$[\sigma(z)’](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e.md)$，参数更新缓慢。**
    > 
    - **交叉熵损失：$L=-yln\hat{y}-(1-y)ln(1-\hat{y}）$，其中$\hat{y}=\sigma(wx+b),z=wx+b$，$w$梯度计算为(链式法则）：**
    
    $$
    \begin{aligned}\frac{\partial L}{\partial w}&=(-y\frac{1}{\hat{y}}+\frac{(1-y)}{1-\hat{y}})*\sigma(z)'*x\\&=-y\frac{1}{\hat{y}}*\sigma(z)'*x+\frac{(1-y)}{1-\hat{y}})*\sigma(z)'*x\\&=-y\frac{1}{\sigma(z)}*\sigma(z)'*x+\frac{(1-y)*\sigma(z)'}{1-\sigma(z)}*x\\&=-y\frac{1-\sigma(z)}{\sigma(z)'}*\sigma(z)'*x +(1-y)\sigma(z)*x\\&=(\sigma(z)-y)*x\end{aligned}
    $$
    
    > 其中不包含$\sigma(z)’$，$\sigma(z)-y$**表示真实值与预测值之间的误差，当误差大，权重更新快。**
    > 
    
- ****MSE+Sigmoid是非凸优化问题****
    
    线性回归使用的是均方误差损失：
    
    $$
    \begin{aligned}
    L_{(\omega, b)} &=\frac{1}{2 N} \sum_{i=1}^{N}\left(f\left(x_{i}\right)-y_{i}\right)^{2} 
    =\frac{1}{2 N} \sum_{i=1}^{N}\left(y_{i}-\omega x_{i}-b\right)^{2}
    \end{aligned}
    $$
    
    因为这个函数$L(w,b)$是凸函数，直接求导等于0，即可求出解析解。**但是对于逻辑回归则不行(分类问题为sigmod）**：
    
    $$
    \begin{aligned}L_{(\boldsymbol{w})} &=\frac{1}{2 N} \sum_{i=1}^{N}\left(y_{i}-\sigma(\boldsymbol{w} \cdot \boldsymbol{x})\right)^{2} \\&=\frac{1}{2 N} \sum_{i=1}^{N}\left(y_{i}-\frac{1}{1+e^{-(\boldsymbol{w} \cdot \boldsymbol{x})}}\right)^{2}\end{aligned}
    $$
    
    ![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/损失函数/Untitled%201.png?raw=true)
    
    > 上式是**非凸的**，不能直接求解析解，而且不宜优化，易陷入局部最优解，即使使用梯度下降也很难得到全局最优解。
    > 

### 回归不用CE

在一个三分类模型中，模型的输出结果为（a,b,c)，而真实的输出结果为(1,0,0)，那么MSE与Cross-entropy相对应的损失函数的值如下

MSE：$c=(a-1)^{2}+(b-0)^{2}+(c-0)^{2}=(a-1)^{2}+b^{2}+c^{2}$

Cross-entropy：$c=(-1) \cdot \log (a)-0 \cdot \log (b)+0 \cdot \log (c)=-\log (a)$

> **交叉熵只和分类正确的预测有关系，而MSE还和错误的分类有关系**，在分类问题中是没有必要的，但回归则需要。
> 

💡 **2、交叉熵函数与最大似然函数的关系和区别？**

- 对数似然函数衡量在某个参数下，**整体的估计和真实情况**一致的概率**，**而损失函数衡量**预测概率分布与真实概率分布**的差距。
- **最小化交叉熵**的本质就是**对数似然函数的最大化**；

💡 **Cross Entropy VS Hinge Loss**

Hinge loss更**关注难以区分**的样本；

Cross Entropy关注**全局所有样本**，尽可能提高正确分类概率。


# Hinge Loss

它通常用于"**maximum-margin**"的分类任务中，$\hat{y}=\pm1$

$$
L(y)=\max (0,m-\hat{y} y)
$$

如果我们希望训练的是两个样本之间的相似关系，$y$是正样本的得分，$y'$是负样本的得分，$m$是margin

$$
l\left(y, y^{\prime}\right)=\max \left(0, m-(y-y^{\prime})\right)
$$

# Focal Loss

**样本不平衡问题；关注难分样本。**

$$
L_{f l}=\left\{\begin{array}{ll}-(1-\hat{p})^{\gamma} \log (\hat{p}) & \text { if } \mathrm{y}=1 \\-\hat{p}^{\gamma} \log (1-\hat{p}) & \text { if } \mathrm{y}=0\end{array}\right.
$$