# 激活函数

<aside>
💡 **为什么需要激活函数？
数据的分布大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，就是引入非线性，强化网络的学习能力。**

</aside>

## Sigmoid

**sigmoid函数取值范围在(0,1)**

<aside>
💡 **优缺点**

**优点：平滑**，**幂函数容易求导**

**缺点：**

- **激活函数计算量大（存在幂运算和除法）**
- **梯度消失，Sigmoid导数取值范围是[0,0.25]，由于神经网络反向传播时的“链式反应”，容易易出现梯度消失。**
- **Sigmoid的输出不是0均值（即zero-centered），收敛慢。**
    
    **假设现在有一个neuron, 内部是这样的：**
    
    ![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e/Untitled.png)
    
    ![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e/Untitled%201.png)
    
    **对W求导：**
    
    $$
    \frac{\partial L}{\partial w}=\frac{\partial L}{\partial f} \frac{\partial f}{\partial w}=\frac{\partial L}{\partial f}x
    $$
    
    **当$x$全为正或者全为负时，每次返回的梯度都只会沿着一个方向发生变化，出现zig-zag path**
    
</aside>

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e/Untitled%202.png)

## T**anh**

**与sigmoid相似，值域在(-1,1)，可以把tanh看做是sigmoid向下平移和拉伸。**

$$
\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$

<aside>
💡 **优缺点**

**优点：**解决了**Sigmoid函数的不是zero-centered**输出问题

**缺点：**

- **幂运算**的问题仍然存在
- **梯度消失**较sigmoid有缓解
</aside>

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e/Untitled%203.png)

## ReLU

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20653a5b6727454907bf247af134ac097e/Untitled%204.png)

<aside>
💡 **优缺点**

**优点：**

- **计算简单，导数为常数不会出现梯度消失，但梯度下降的强度取决于模型权值**
- **稀疏权重矩阵**。

**缺点：**

- 导致大部分神经元**处于‘dead’状态**
- ReLU可能出现**梯度爆炸**
- 不是**zero-centered**
</aside>

**变种：**

- Leaky ReLu，将x<0部分不置为0，而给一个很小的负数梯度值α
- PRelu（参数化修正线性单元），α作为一个可学习的参数进行更新。
- RReLU（随机纠正线性单元），训练时α是随机的（均匀的分布中抽取），测试时固定。