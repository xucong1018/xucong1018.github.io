# 逻辑回归

## Linear Regression

**线性回归用于回归任务，拟合输入和输出之间的线性关系，均方误差作为损失函数，最小损失函数的方法包括最小二乘法、梯度下降。**

📎 **最小二乘法**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled.png?raw=true)

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%201.png?raw=true)

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%202.png?raw=true)


💡 **适用环境**

- 自变量和因变量之间是线性关系时
- 每一维之间没有共线性（人为组合、降维）
    
    ![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%203.png?raw=true)
    
    **自变量之间是高度相关关系，会使得矩阵的行列式近似等于零，存在多个解析解，模型不稳定。**
    

💡 **优缺点**

- 直接、快速、**可解释性好**。
- 对**异常值**很敏感

## Logistic Regression

[逻辑回归(Logistic Regression)详解](https://zhuanlan.zhihu.com/p/56900935)

**逻辑回归是分类模型，但本质上又是线性回归模型，除去sigmoid（逻辑回归函数）映射函数，其他的步骤，算法都是线性回归的。**

💡 **极⼤似然的核⼼思想：如果现有样本可以代表总体，那么极大似然估计就是找到⼀组参数使得出现现有样本的可能性最⼤。**

**逻辑回归假设数据服从伯努利分布，构造极⼤化似然函数的⽅法，利⽤梯度下降求解参数;**

$$
似然函数：l=\prod_{i=1}^{m}\left[\pi\left(x^{(i)}\right)\right]^{y^{(i)}}\left[1-\pi\left(x^{(i)}\right)\right]^{1-y^{(i)}}\\对数似然：L(w)=\sum_{i=1}^{m}\left[y^{(i)} \log \pi\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-\pi\left(x^{(i)}\right)\right)\right]
$$

**逻辑回归损失函数使用交叉熵**

$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_\theta(x)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}(x)\right)\right]
$$

**对$J(\theta)$求最小，即对对数似然函数求极大**。


💡 **优缺点**

- **优点**
    1. **直接预测分类的概率**，从而预测出类别，
    2. 因为结果是概率，可**用作排序**
    3. **对率函数是任意阶可导凸函数**，有很好得数学性质，
- **缺点**
    1. **容易欠拟合**，表达能力有限
    2. **数据特征有缺失或特征空间很大**时效果不好

💡 **LR一般需要连续特征离散化原因**

- **离散特征稀疏计算快**，易于模型快速迭代
- **离散化的特征对异常数据有很强的鲁棒性**(比如年龄为300异常值可归为年龄>60区间)
- **特征离散化后，模型会更稳定**（比如对用户年龄离散化，20-30作为一个区间，不会因为用户年龄，增加一岁变成完全不同的人，但区间相邻处样本会相反，所以划分区间很重要）
- **提升模型表达能力**；单变量离散化后每个变量有单独的权重，相当于引入了非线性，

💡 **逻辑回归的Sigmoid的输出可以表示概率么？**

**sigmoid** 本身不能赋予输出概率意义，⽽是 **Cross Entropy** 这样有概率意义的损失函数，让输出可以解释成概率



💡 **逻辑回归为什么⽤sigmoid？**

由⼴义线性模型做⼆分类，变量服从伯努利分布，可以推导得到sigmoid的形式，

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%204.png?raw=true)


💡 **逻辑回归 VS 线性回归**

**相同之处：**

- 都使用了**极大似然估计**来对样本建模。线性回归利用最小二乘法求解，而逻辑回归通过对似然函数的学习，得到最佳参数。
- 梯度下降优化

**不同之处：**

- 逻辑回归处理的是**分类问题**，线性回归处理的是**回归问题**
- 线性回归要求**自变量和因变量呈线性关系**，而logistic回归不要求
- logistic回归是分析因变量**取值的概率与自变量的关系**，而线性回归是**直接分析因变量与自变量的关系**

💡 **逻辑回归 VS SVM**

**相同之处：**

- 都是有监督的**分类算法**；
- **分类决策面**都是线性的

**不同之处：**

- LR是**参数模型**，SVM是**非参数模型**。（假设数据分布，参数指**数据分布的参数）**
- 从**损失函数**来看，逻辑回归采用的是logistical loss，SVM采用的是hinge loss
- SVM只考虑分界⾯附近的**少数点**（支持向量），⽽LR则考虑**所有点**。

## SVM

[看了这篇文章你还不懂SVM你就来打我](https://zhuanlan.zhihu.com/p/49331510)

**SVM学习的基本想法是找到能够正确划分类别并且间隔最大的超平面。**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%205.png?raw=true)

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/逻辑回归/Untitled%206.png?raw=true)

## Navie Bayes

**朴素贝叶斯算法是一种基于贝叶斯定理的分类方法，它假设在预测变量之间具有独立性。**


💡 **原理**



通常分类器是一个**条件概率模型**，目标是计算：

$$
p(C \mid F 1, \ldots, F n) = P(C,F_1,\cdots, F_n)/P(F_1,\cdots, F_n)
$$

但如果**特征数量较大或者每个特征有大量取值**，计算联合概率分布不现实。

因此根据**贝叶斯定理**可以将上式转化为：

$$
p(C \mid F 1, \ldots, F n)=\frac{p(C) * p\left(F 1, \ldots F_{n} \mid C\right)}{p\left(F 1, \ldots, F_{n}\right)}
$$

朴素的条件：假设每个特征独立，因此

$$
p(C \mid F 1, \cdots, F n)=\frac{p(C) * p(F_1|C)*\cdots * p(F_{n}|C)}{p(F 1, \cdots, F_{n})}
$$