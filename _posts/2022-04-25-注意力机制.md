# 注意力机制

[深度理解Attention机制](https://zhuanlan.zhihu.com/p/388913620)

**注意力机制可以让模型对重要信息重点关注并吸收的技术。**

<aside>
💡 **self-Attetion的步骤和作⽤？ 
以一个句子序列为例：**

- **对于句子中的每一个token，分别与三个权重矩阵相乘，得到Q、K、V三个向量表示；**
- **然后计算权重，以句子中第一个词为例，用它的Q向量点积其他词的K向量，并作softmax操作得到权重，这个权重表示在编码第一个词时，其他词的重要性。**
- **通过权重对其他词加权求和。**
</aside>

<aside>
💡 **为什么除以$\sqrt {d_k}$?
Softmax时分母较大，会导致梯度较小，参数更新困难**

</aside>

<aside>
💡 **multi-headed attetion的作⽤
多头机制类似于CNN的”多通道“特征提取，可以让模型关注不同子空间的信息，提高模型的表达能力。**

</aside>

<aside>
💡 **为什么用LN?
Batch Normalization是对每个Batch的每⼀特征做normalization，相当于是同一batch所有句子的相同位置单词做归⼀化，**

**而Layer Normalization是每一层网络做normalization，相当于是对每句话的中的词做归⼀化。显然， LN更加符合我们处理⽂本的直觉。**

</aside>

> **Attention区别**
> 

在⼀般任务的Encoder-Decoder框架中，输⼊Source和输出Target内容是不⼀样的，⽐如对于英-中机器翻译来说， Source是英⽂句⼦， Target是对应的翻译出的中⽂句⼦。

⽽ Self Attention顾名思义，指的不是Target和Source之间的Attention机制，⽽是Source内部元素之间或者Target内部元素之间发⽣的Attention机制。

<aside>
📢 考虑一个非常常见的场景：

假设现在有一个经过Word Embedding的句子矩阵$M_{L*D_e}$ ($L$为句子长度，$D_e$为单词嵌入。矩阵中的每一行向量$V_{W_i}$便可以较好地表示单词$W_i$的语义。（**CBOW等Embedding模型通过引入“词袋”这个概念较好地考虑了较短距离的上下文的语义信息）**

</aside>

<aside>
❓ **如何可以学习到长距离的上下文语义信息呢？
Attention就是做这个事情的。**

</aside>

- **一种简单的方法：求单词与单词之间的相似性，然后加权相加（Attention机制的雏形）。**
    
    求向量相似性的方法有：余弦模型、加性模型、点积模型、双线性模型等等，利用相似性作为权重聚合信息。**无参学习几乎没有学习能力**
    
    ![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled.png)
    

- **Self-Attention（引入参数矩阵，增加可学习性）**
    
    **第一步：**首先对每个单词$W_i$的求其$\overrightarrow{query_{i}}, \overrightarrow{key_{i}}, \overrightarrow{value}_{i}$ 。
    
    ![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%201.png)
    
    **第二步：**计算单词与单词相似性（相关性）的时候，输入是 $\overrightarrow{query_{i}}, \overrightarrow{key_{i}}$ 
    
    **第三步：**最后进行加权相加的时候，权重$softmax$处理后的概率值，而待加权向量为  $\overrightarrow{value}_{i}$
    

![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%202.png)

$\sqrt{D_{e}}$表示缩放因子。避免点积太大，$softmax$后梯度小。

> **Attention 机制的本质**
> 

![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%203.png)

**想象内存里的一块存储空间，它里面存储的数据按 <Key, Value> 存储。给定 Query，然后取出对应的内容。每个地址都只取一部分内容，然后对所有的 Value 加权求和。**

![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%204.png)

<aside>
💡 拓展

![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%205.png)

1）**Soft** Attention，这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。

2）**Hard** Attention，这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）

3）**Local** Attention，这种方式其实是以上两种方式的一个折中。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。

</aside>

<aside>
💡 **Transformer**

- **self-attention机制建模序列为扁平的结构（词袋，bag of words），词之间距离都为1；这样会丢失词之间的相对距离关系。**
    
    为了缓解这个问题，Transformer中将词在句子中所处的位置映射成vector，补充到其embedding中去。将绝对位置作为三角函数中的变量做计算，具体公式如下：
    
    ![Untitled](%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2077e5524373ee4fa19b26f7cf50f317a1/Untitled%206.png)
    
- **Transformer里面的self-attention机制是一种新的变种，体现在两点，**
    
    一方面是加了一个**缩放因子（scaling factor）；**
    
    另一方面是引入了**多头机制（multi-head attention）**，允许模型在不同的表示子空间里学习到相关的信息，如CNN中多个filter。
    
- **free-running VS teacher-forcing**
    
    它每次不使用上一个state的输出作为下一个state的输入，而是直接使用训练数据的标准答案(ground truth)的对应上一项作为下一个state的输入。（training时，把正确结果当做decoder的输入，在testing时，把上一个时刻decoder的结果当做当前时刻的输入，但是不能让decoder在训练时只看到正确结果，就需要在training时加一些扰动。）
    
</aside>

****