# 调参技巧

- Learning Rate设置合理

<aside>
❓ 太大: loss爆炸, 或者nan；太小，训练不动

</aside>

- 自动调参方法

<aside>
❓ Grid Search：遍历所有候选的参数，速度慢
Random Search：随机搜索所有候选的参数

</aside>

- 注意loss的范围

<aside>
❓ 多任务情况下, 各loss限制在一个量级上

</aside>

- 激活函数选择

<aside>
❓ 对于输出层，多分类任务选用softmax输出，二分类任务选用sigmoid输出，回归任务选用线性输出。而对于中间隐层，则优先选择relu激活函数

</aside>

- 好的优化器

<aside>
❓ 一般使用自适应的优化器，如Adam

</aside>