---
layout:     post
title:      Dropout
subtitle:   Dropout
date:       2021-03-28
author:     Cong
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - Dropout
---



# Dropout

[](https://blog.csdn.net/stdcoutzyx/article/details/49022443)

[对dropout的理解详细版_furuit的博客-CSDN博客_dropout](https://blog.csdn.net/fu6543210/article/details/84450890)

**Dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。**

原理就在于，即使一张图像缺少少量部分，但我们人眼依然能分辨出目标。那一张图像可以有不同的地方缺失，这样就如同样本变多了，我们知道训练样本越大，网络越不容易拟合。

### ****观点一：从模型****

大规模的神经网络有两个缺点：

- 费时
- 过拟合

> **为了解决过拟合问题，一般会采用ensemble方法——>耗时**
> 

做Dropout相当于每次训练一个”更瘦“的网络

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/Dropout/Untitled.png?raw=true)

> **对于一个有N节点的神经网络，做了Dropout相当于有了$2^n$个模型集合，但总的参数没变。**
> 

> 它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。
> 

> 0.5的时候dropout随机生成的网络结构最多（$A_n^{n/2}$）
> 

### ****观点二：从数据****

**对于dropout后的网络，进行训练时，相当于做了Data Augmentation，**因为，总可以找到一个样本，使得在原始的网络上也能达到dropout单元后的效果。 比如，对于某一层，dropout一些单元后，形成的结果是(1.5,0,2.5,0,1,2,0)，其中0是被drop的单元，

### **训练和测试的差异？**

训练神经元只有$(1-p)$的概率参与，测试要降低规模：假设一个神经元输出为$x$，训练时有$p$的概率被抛弃，那么输出的期望为$0*p+(1-p)x=(1-p)x$，因此测试时将这个神经元权重乘$(1-p)$即可。
- 测试时每个神经元的输出结果乘$(1-p)$。
- **inverted dropout：**训练时留下的神经元的权重除$(1-p)$，测试不做处理

✅ **拓展：**

- **高斯Dropout**
    
    ![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/Dropout/Untitled%201.png?raw=true)
    
    数学上的表示如上所示，有一个高斯mask的乘法（**以1为中心，标准差$p(1-p)$）**。
    
    > 保持所有的神经元都是活跃的，随机地对其预测能力进行加权，模拟了dropout。
    > 
    
    > 优势是在测试阶段，不需要进行任何修改。
    > 
