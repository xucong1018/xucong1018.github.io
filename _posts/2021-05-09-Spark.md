---
layout:     post
title:      Spark
subtitle:   Spark简介
date:       2021-05-09
author:     Cong
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - Spark
---


# Spark

Spark是一种快速、通用、可扩展的大数据分析引擎，高效地处理分布式数据集。

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/Spark/Untitled.png?raw=true)

Spark主要包括Spark RDD、spark SQL、Spark Streaming，Spark 机器学习等组件，我主要负责的是spark实验指导的spark 机器学习部分的编写，主要是利用spark ML组件实现一些常用的分类，回归，聚类方法，并设计相关实验（股票涨跌预测，价格预测等。）

分布式结构是master slave模式，master 控制集群的运行， slave是计算节点，

**本地模式**

**独立集群模式（standalone) master/slave**

**yarn资源管理器  与其他分布式应用共享集群就需要yarn，进行资源分布，资源利用低下**

## **RDD**

- RDD( Resilient Distributed Dataset，弹性分布式数据集)是一个分布式数据集的抽象，本质上是逻辑分区记录的集合。
- 可以让用户显式地将数据存储到磁盘和内存中，并且还能控制数据的分区。
- 对于迭代式计算，RDD可以将中间计算的数据结果保存在内存中，后面需要中间结果时，则可以直接从内存中读取，从而可以极大地提高计算速度。

### 特性

> **分区（partitions）**
> 

RDD划分成多个分区（partitions）分布到集群的节点上，一个RDD可以包含多个分区，每个分区是一个dataset片段。

> **分区策略**
> 

Spark实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外个是基于范围的RangePartitioner。只有对于(Key，Value)的RDD，才会有Partitioner。Partitioner函数不但决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出时的分区数量。

> **每个分区有一个计算函数**
> 

Spark RDD的计算是以分区为基本单位的，每个RDD都会实现compute函数。compute函数会对迭代器进行复合，不需要保存每次计算的结果。**compute方法调用当前RDD内的第一个父RDD的iterator方法，拉取父RDD对应分区的数据。**

> **依赖于其他RDD**
> 

RDD之间会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过依赖关系重新计算丢失的分区数据，而不是对所有分区重新计算。

> **每个分区都有一个优先位置列表**
> 

优先位置列表存储存取每个Partitioner的优先位置，”数据不动代码动“，尽量地将任务分配到要处理数据块的存储位置。

### **依赖关系**

RDD的依赖关系有两种类型：

窄依赖：父RDD的每一个分区最多被一个子RDD的分区使用（one to one

宽依赖：多个父RDD的分区对应于一个子RDD的分区

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/Spark/Untitled%201.png?raw=true)

### RDD操作

RDD有两种操作算子：**Transformation（转换）操作**和**Action（行动）操作**。Transformation都具有**Lazy**特性，不立即计算RDD的结果，仅记录转换操作应用到哪些RDD，Transformation仅仅在执行Action时才进行计算。

- Transformation（转换）操作是从已经存在的数据集上创建一个新的数据集。

| Transformation操作 | 含义 |
| --- | --- |
| map | 数据集中的每个元素经过func函数转换后形成一个新的分布式数据集 |
| filter | 过滤函数，选取数据集中让函数func返回值为true的元素，形成一个新的数据集。 |
| flatMap | 类似map方法，但每一个输入项可以被映射为0个或者多个的输出项。返回的是Seq |
| union | 返回一个由原数据集和参数数据集联合（求并集）而成的新的数据集  |
| distinct | 返回一个数据集去重后得到的新的数据集。 |
| groupByKey | 按照key进行分组，返回一个键值对的数据集。 |
| reduceByKey | 按照key将数据分组，使用给定的func聚合values值， |
| sortByKey  | 返回一个以key排序（升序或者降序）的（K，V）键值对组成的数据集 |
|  join | 根据key连接两个数据集 |
- action算子会触发SparkContext提交Job

| Action算子 | 含义 |
| --- | --- |
| reduce(func) | 通过函数func聚集数据集中的所有元素 |
| aggregate | 将每个分区里面的元素进行聚集 |
| collect() | 以数组的形式返回数据集的所有元素。 |
| count() | 返回数据集的元素个数。 |
| first() | 返回数据集的第一个元素。  |
| take(n) | 以数组的形式，返回数据集上的前n个元素。 |
| foreach(func) | 在数据集的每个元素上都运行func函数。 |
| countByKey() | 只能运行在键值对类型（K，V）上，对每个key相等的元素个数进行计数。 |
| saveAsTextFile(path) | 将数据集的元素作为一个文本文件（或文本文件的集合）保存在本地文件系统中的给定目 |

### 缓存机制（持久化）

Action操作会触发一次从开始到结尾的运算，对于迭代运算代价是很大，迭代计算经常需要使用上一次运算的结果或者同一组数据。为了避免重复计算的开销，就涉及到持久化（缓存）机制，**即第一次行动操作得到的结果，如果能被第二次行动操作使用，则不需要从头开始计算。**

1. 通过使用**persist()**或者**cache()**方法持久化的RDD，使其被保存在内存的节点上
2. cache()方法默认为将RDD缓存在内存中，实际上是调用persist(MEMORY_ONLY)方法。
3. 缓存机制是容错的，RDD的任意分区丢失，将会重新计算丢失的部分。

### 检查点容错机制

**为了避免缓存丢失重新计算带来的开销，Spark引入检查点（Checkpoint）机制。**

Checkpoint的产生就是为了相对而言**更加可靠的持久化操作**，在Checkpoint可以指定把数据放在本地并且是**多副本方式。**(正常的生产环境是放在 HDFS)