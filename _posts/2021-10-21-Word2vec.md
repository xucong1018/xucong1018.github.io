# NLP

## Word2vec

**Word2vec使语义相似的词在特征空间中处在相近的位置。包括连续词袋模型（CBOW） 与跳字模型（Skip-Gram）** 

![Untitled](NLP%20766f558b6db040748d4bf48cc8e4edb0/Untitled.png)

<aside>
💡 **步骤**

**对 CBOW 而言，它的输入是目标特征词的上下文词语的独热编码（one-hot），然后训练获得词嵌入模型。** 

![Untitled](NLP%20766f558b6db040748d4bf48cc8e4edb0/Untitled%201.png)

- 首先输入目标词上下文的**独热编码**。初始化一个可学习的**输入权重矩阵**，将输入分别与输入权重矩阵做矩阵相乘，并**求和平均得到隐藏层表达**。
- 同时初始化一个**输出权重矩阵**，将隐藏层表达与输出权重矩阵相乘，并使用激活函数处理得到模型输出向量，其中**最大值所处的维度指向目标词**，
- 将**输出向量与目标词的独热编码计算损失**优化， 连续词袋模型的损失计算为**交叉熵函数**。在训练模型后，最终需要的是**隐藏层中的输入权重**。
</aside>

## Transformer

[熬了一晚上，我从零实现了Transformer模型，把代码讲给你听](https://zhuanlan.zhihu.com/p/411311520)

T**ransformer encoder**的组成： **self-Attention**和**feed forward**组成，中间穿插残差⽹络

![Untitled](NLP%20766f558b6db040748d4bf48cc8e4edb0/Untitled%202.png)