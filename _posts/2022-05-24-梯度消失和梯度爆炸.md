# 梯度消失和梯度爆炸

[梯度消失和梯度爆炸及解决方法](https://zhuanlan.zhihu.com/p/72589432)

# 一、**为什么会产生梯度消失和梯度爆炸？**

**神经网络的误差通过链式法则（Chain Rule）反向传播。**

**链式法则是一个连乘的形式，所以当层数越深，梯度将以指数形式传播，得到的梯度可能接近0或特别大，也就是发生了梯度消失或爆炸。**

# 二、**分析产生梯度消失和梯度爆炸的原因**

## **【梯度消失】**

产生的原因有：一是在**深层网络**中，二是采用了**不合适的激活函数**，比如sigmoid。

**现象：**当梯度消失发生时，接近于**输出层的隐藏层由于其梯度相对正常**，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会**导致靠近输入层的隐藏层权值更新缓慢或者更新停滞**。这就导致在训练时，只有后面几层网络在学习。

## **【梯度爆炸】**

一般出现在**深层网络**和**权值初始化值太大**的情况下。

**现象：**在深层神经网络或循环神经网络中，**误差的梯度可在更新中累积相乘**。如果网络层之间的**梯度大于 1.0**，那么**重复相乘会导致梯度呈指数级增长**，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。

**①模型不稳定，导致更新过程中的损失出现显著变化；②训练过程中，在极端情况下，权重的值变得非常大，损失变成 NaN等。**

# 三、根本原因

## （1）**深层网络**

一个比较简单的深层网络如下：

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled.png)

链式法则是一个连乘的形式，所以当**层数越深的时候，梯度将以指数传播。**

如果接近**输出层的激活函数求导后梯度值大于1**，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生**梯度爆炸**；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生**梯度消失**。

## （2）**激活函数**

以下图的反向传播为例（假设每一层只有一个神经元且对于每一层，其中$y_i=\sigma(z_i) = \sigma (w_ix_i+ b)$,其中$\sigma$为sigmoid函数为sigmoid函数）。

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%201.png)

可以推导出：

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%202.png)

原因看下图，sigmoid导数的图像。

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%203.png)

如果使用sigmoid作为损失函数，其梯度是**不可能超过0.25**的，而我们初始化的网络权值
通常**都小于1**，因此$|\sigma^{'}(z)w|<1/4$，因此对于上面的链式求导，**层数越多**，求导结果$\frac{\partial C}{\partial b_1}$
**越小**，因而很容易发生**梯度消失**。

## （3）**初始化权重的值过大**

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%204.png)

如上图所示，**当$|\sigma^{'}(z)w|>1$，也就是$w$比较大的情况**。根据链式相乘(反向传播)可得，则**前面的网络层比后面的网络层梯度变化更快**，很容易发生**梯度爆炸**的问题。

# 四、**解决方法**

梯度消失和梯度爆炸问题都是因为**网络太深**，**网络权值更新不稳定**造成的，本质上是因为**梯度反向传播中的连乘效应**。解决梯度消失、爆炸主要有以下几种方法：

## （1）预训练+微调

**预训练**就是指预先训练一个模型，用于下游任务的模型初始化；

**微调**就是将预训练的模型作用于自己的数据集，并使参数适应自己数据集；分为只训练最后分类层 and 完全训练

## **（2） 梯度剪切：对梯度设定阈值**

主要是针对**梯度爆炸**，其思想是设置一个**梯度剪切阈值**，更新梯度的时候，如果梯度超过这个阈值，那么就将其强制**限制在阈值之内。**。

## **（3） 正则化**

另外一种解决梯度爆炸的手段是采用**权重正则化（weithts regularization）**，通过**正则化项来限制权重的大小**，也可以在一定程度上防止梯度爆炸的发生。比较常见的是 L1 正则和 L2 正则，

## **（4） 选择relu等梯度大部分落在常数上的激活函数**

relu函数的导数在正数部分是**恒等于1**的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

## **（5） Batch Normalization**

BN就是通过对**每一层的输出规范为均值和方差一致**的方法，消除了**权重参数放大缩小**带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从从**饱和区**拉倒了**非饱和区**。

## **（6） 残差网络的捷径（shortcut）**

相比较于以前直来直去的网络结构，残差中有很多这样（如上图所示）的**跨层连接结构**，这样的结构在反向传播中具有很大的好处，可以避免梯度消失。

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%205.png)

## **（7） LSTM的“门（gate）”结构**

LSTM 通过它内部的“门”可以在接下来更新的时候**“记住”**前几次训练的”**残留记忆**“。

![Untitled](%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%2004edb906a69246d8b227d5bfd1fa505c/Untitled%206.png
$y=f(x)$