# 决策树

# Decision Tree

[决策树算法](https://zhuanlan.zhihu.com/p/268619723)

[【机器学习】决策树（上）--ID3、C4.5、CART（非常详细）](https://zhuanlan.zhihu.com/p/85731206)

**决策树就是按照信息增益不断提纯数据的过程，在样本的某个特征下进行划分，使得划分后的数据不确定性不断降低。**

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled.png)

## ID3

**使用信息增益构建决策树就是ID3算法，信息增益准则偏向于选择取值较多的特征。**

特征A对训练数据集D的信息增益定义为：D的信息熵与A条件下的条件熵之差，即

$$
g(D, A)=H(D)-H(D \mid A）
$$

其中 $H(D)=-\sum_{i=1}^{k} \frac{\left|D^{i}\right|}{|D|} \log \frac{\left|D^{i}\right|}{|D|}$， $k$为类别数；$H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D^{i}\right|}{|D|} H\left(D^{i}\right)$ ，$n$为特征A的取值个数。

💡 **评价**

- 信息增益准则对取值数目较多的特征有所偏好
- 只能用于处理离散分布的特征；
- ID3 没有剪枝策略，容易过拟合；
- 没有考虑缺失值。

## C4.5

为了解决ID3偏向于取值较多的属性。C4.5是基于信息增益率进行划分（**多了会随着属性取值数量增多而变大的分母**）

$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
$$

其中$H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log \frac{\left|D_{i}\right|}{|D|}$，n为特征A取值数。

💡 **评价**

- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有**大量耗时的对数运算，连续值还有排序运算**；

## ****CART (Classification and Regression Tree)****

CART既可以用于分类，也可用于回归。分类树用使用Gini系数作为衡量标准，对回归树用最小化**平方误差**。

### 基尼指数（纯度）

> 基尼指数反映了从**数据集中随机抽取两个样本，其类别不一致的概率**。因此基尼指数越小，则数据集纯度越高。基尼指数**偏向于特征值较多的特征**，类似信息增益。**类别越杂乱，GINI指数就越大**
> 
- 分类问题中，假设有k个类，样本点属于第i类的概率为$p_i$，则概率分布的基尼指数定义为

$$
Gini(p)=\sum_{i=1}^{k} p_{i}\left(1-p_{i}\right)=1-\sum_{i=1}^{k} p_{i}^{2}
$$

- 对于给定的样本集合D，其基尼指数为
    
    $$
    Gini(D)=1-\sum_{i=1}^{k}\left(\frac{\left|D_{i}\right|}{|D|}\right)^{2}
    $$
    
    其中， $D_i$是$D$中属于第$i$类的样本集合， $k$是类的个数。
    
    如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即$D$则在特征$A$的条件下，集合D的基尼指数定义为
    
    $$
    Gini(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gin} i\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} Gini\left(D_{2}\right)
    $$
    

基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D, A)$表示经$A=a$分割后集合$D$的不确定性。

💡 **评价**

- C4.5 只能**分类**，CART 既可以**分类也可以回归**；
- C4.5 为**多叉树**，运算速度慢，**CART 为二叉树**，运算速度快；
- CART 使用 Gini 系数作为切分标准，**减少了对数运算**；
- CART 采用**代理分裂器**来估计缺失值（**代理分裂器就是代替缺失特征作为划分特征的特征**），而 C4.5 以**不同概率划分缺失特征**到不同节点中；
- **CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。**

## 📌**延伸提问**

💡 **1、差异总结**

- **划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，**CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。**
- **使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
- **样本数据的差异：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值； CART 使用代理分裂器，C4.5以不同权重划分缺失值到不同分支**
- **样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
- **剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

💡 **2、如何解决决策树的过拟合问题？**

> **预剪枝：在构建树的过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树性能提升，则停止划分。**
> 
> 
> **采用留出法（预留一部分数据作验证集）。**
> 
- 优点：降低了过拟合风险，减少了训练时间和测试时间开销。
- 缺点：欠拟合风险。

> **后剪枝是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察。
若将该结点对应子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。**
> 
- 优点：欠拟合风险小，泛化能力优于预剪枝。
- 缺点：训练时间开销大。

> **代价复杂度**
> 

我们希望减少树的大小来防止过拟合，但又担心去掉节点后预测误差会增大，所以我们定义了一个损失函数来达到这两个变量之间的平衡。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%201.png)


💡 3、****连续值与缺失值处理****

- **连续值处理：二分法**
    
    在训练集D中，假定连续属性a共有n个不同的取值，将这些值从小到大进行排序，记为$\left\{a^{1}, a^{2}, \ldots, a^{n}\right\}$，我们把每个小区间$[a^i,a^{i+1})$的中位点$\frac{a^{i}+a^{i+1}}{2}$ 作为候选划分点，只要在这n-1个点中找到一个最优的即可，也就是要找到一个使得信息增益最大的划分点。
    
    与离散属性不同，**若当前结点划分属性为连续属性，该属性仍可作为其后代结点的划分属性**（如父结点使用“密度>0.381”，子结点也可能使用“密度>0.563”）.
    
- **缺失值处理**
    
    面临的两个问题是（**1）**如何选择要划分特征？（**2）**若属性缺失，如何对样本划分？
    
    先对训练集中每个样本赋予一个权重初始化为1。
    
    **a. 特征选择：信息增益加权**
    
    **按不缺失的计算出信息增益，再按样本比例赋上一个权重**。可描述为：在训练集D中，$D^{\prime}$为属性A上没有缺失值的样本子集，则该属性的信息增益为：
    
    $$
    g(D, a)=\rho \times g\left(D^{\prime}, a\right)\\\rho=\frac{\sum_{x \in D^{\prime}} w_{x}}{\sum_{x \in D} w_{x}}
    $$
    
    **b. 样本划分：加权划分**
    
    若样本$x$在该属性不缺失，则将其划入对应的子结点，且样本权值$w_x$在子结点中保持不变。
    
    若样本$x$在该属性缺失，**则将其以样本比例乘以权值$\frac{\sum_{x \in D^{\prime} v} w_{x}}{\sum_{x \in D^{y}} w_{x}} \cdot w_{x}$同时划分到所有子结点。**
    

# 集成学习

常见的集成学习框架有三种：**Bagging**，**Boosting** 和 **Stacking**。

## ****Bagging****

Bagging 全称叫 **B**ootstrap **agg**regat**ing，**每个基学习器都会对训练集进行**有放回抽样**得到子训练集。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%202.png)

## Boosting

Boosting 训练过程为**阶梯状**，基模型的训练是有顺序的，**每个基模型都会在前一个基模型学习的基础上进行学习**，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%203.png)

## Stacking

Stacking 是先用**全部数据**训练好基模型，然后每个基模型都对每个训练样本进行的预测，其**预测值将作为训练样本的特征值**，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%204.png)

通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型，但并不是所有集成学习框架中的基模型都是弱模型。

**Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高），而Boosting 中的基模型为弱模型（偏差高，方差低）**。

# **Random Forest**

**Random Forest（随机森林），用随机的方式建立一个决策树森林。每一棵决策树之间没有关联。建立完森林后，当新样本进入时，每棵决策树都会进行判断，然后基于投票法给出分类结果。**

## 思想

Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了**随机特征选择**，因此可以概括 RF 包括四个部分：

1. **随机选择样本（放回抽样）**；
2. **随机选择特征；**
3. 构建决策树；
4. 随机森林投票（平均）。

> **随机选择样本**采用有放回的抽样；**随机选择特征**是指在每个节点在分裂过程中随机选择特征的。采用两种随机方法，不剪枝也不会出现过拟合。
> 

💡 **随机选择特征目的？**

随机选择特征是指**随机选择部分特征构建树**，这种**随机性**导致随机森林的**偏差**稍微增加，但是由于随机森林的**‘平均’特性**，会使得它的方差减小，防止过拟合。


## **优缺点**

- **优点**
1. 能够处理**高维度数据**，不用做特征选择。
2. 易于**并行化**，在大数据集上有很大的优势；
3. 不容易**过拟合**
- **缺点**
1. **取值划分较多的属性会对随机森林产生更大的影响，**所以产出的属性权值是不可信的。
2. 在**某些噪音较大**的数据集上会过拟合。

# Adaboost

AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：**前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器，直到达到足够小的错误率或达到最大迭代次数。**

$$
H_{t}(x)=H_{t-1}(x)+\alpha_{t} h_{t}(x)
$$

❓ **加法模型**

Boosting类的模型大多数都是加法模型。什么是加法模型呢？简而言之，就是模型的输出是一组函数的加和的形式：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%205.png)

其中，T表示**基学习器**的数量，$f_t$表示第t个学习器模型。如上的公式表示GBDT的模型。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%206.png)


💡 **思想**

Adaboost 迭代算法有**三步**：

1. 初始化训练样本的权值分布，每个样本具有**相同权重**；
2. 训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会**被降低**；反之提高。用更新过的样本集去训练下一个分类器；
3. 将**所有弱分类组合成强分类器；**各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。

💡 **优缺点**

- **优点**
    1. 分类**精度高**；不容易发生**过拟合**
    2. 灵活，**可以用各种回归分类模型来构建弱学习器**
    3. 对**异常点敏感**，异常点会获得**较高权重**。

# ****GBDT(Gradient Boosting Decision Tree)****

[机器学习-一文理解GBDT的原理-20171001](https://zhuanlan.zhihu.com/p/29765582)

**GBDT模型是一个加法模型，它串行地训练一组CART回归树，并对所有回归树的预测结果加和得到最终结果，每一颗新树都拟合当前损失函数的负梯度方向。**

- **GBDT 的每一步残差计算变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0**
- GBDT的每个基模型都是一颗CART回归树。也就是说，训练GBDT模型是用**同样的数据和特征**，**不同的标签（当前负梯度）**，**串行**地训练多个CART回归树，输出是对这些CART回归树结果加和。

💡 **每个学习器的目标时拟合之前所有学习器留下的残差(负梯度）。**

假设我们已经有了$n-1$个学习器的组合：

$$
F_{n-1}(x)=\sum_{i=1}^{n-1} f_{i}(x)
$$

下一个最好基学习器$f_n(x)$应该把$y-F_{n-1}(x)$看做新的拟合目标；之后，可以得到一个新的更强的学习器：

$$
F_{n}(x)=F_{n-1}(x)+f_{n}(x)=y
$$

根据梯度下降的思想，假设有一损失函数$J(\theta)$，当我们需要参数为$\theta$的损失函数求最小值时，我们按照损失函数的负梯度方向调整参数 即可。

$$
\theta:=\theta-\rho \frac{\partial J}{\partial \theta}
$$

同理，把$\theta$换成模型$F_n(x)$，则有

$$
F_n(x)=F_{n-1}(x)-\frac{\partial L\left(y, F_{n-1}(x)\right)}{\partial F_{n-1}(x)}
$$

于是可以知道基学习器$f_n(x)$本质上实在拟合的是损失函数的负梯度；

> **当损失函数使用平方误差损失函数，则可以用损失函数的负梯度作为残差。推广到其他损失函数，利用损失函数的负梯度作为残差近似值。**
> 

**损失函数为：**

$$
L(y, F(x))=\frac{1}{2}(y-F(X))^{2}
$$

**最小化损失函数，这里利用梯度下降的想法，对损失函数求一阶导：**

$$
\frac{\partial J}{\partial F\left(X\right)}=\frac{\partial (\frac{1}{2}(y-F(X))^{2} )}{\partial F\left(X)\right)}=F(x)-y
$$

**正好残差就是负梯度：**

$$
y-F(X)=-\frac{\partial J}{\partial F(X)}
$$


💡 **另一种解释（泰勒展开）**

对$f(x)$在$x=x_{t-1}$处泰勒展开为：$\mathrm{f}(\mathrm{x}) \approx \mathrm{f}\left(\mathrm{x}_{\mathrm{n}-1}\right)+\mathrm{f}^{\prime}\left(\mathrm{x}_{\mathrm{n}-1}\right)\left(\mathrm{x}-\mathrm{x}_{\mathrm{n}-1}\right)$，因此对于损失函数$L(y,F(x))$在$F(x)=F_{n-1}(x)$处泰勒展开则有：

$$
\mathrm{L}(\mathrm{y}, \mathrm{F}(\mathrm{x})) \approx \mathrm{L}\left(\mathrm{y}, \mathrm{F}_{\mathrm{n}-1}(\mathrm{x})\right)+\left[\frac{\delta \mathrm{L}(\mathrm{y}, \mathrm{F}(\mathrm{x}))}{\delta \mathrm{F}(\mathrm{x})}\right]_{\mathrm{F}(\mathrm{x})=\mathrm{F}_{\mathrm{n}-1}(\mathrm{x})}\left(\mathrm{F}(\mathrm{x})-\mathrm{F}_{\mathrm{n}-1}(\mathrm{x})\right)
$$

将$F(x)=F_{n}(x)$代入上式有：

$$
\mathrm{L}\left(\mathrm{y}, \mathrm{F}_{\mathrm{n}}(\mathrm{x})\right) \approx \mathrm{L}\left(\mathrm{y}, \mathrm{F}_{\mathrm{n}-1}(\mathrm{x})\right)+\left[\frac{\delta \mathrm{L}(\mathrm{y}, \mathrm{F}(\mathrm{x}))}{\delta \mathrm{F}(\mathrm{x})}\right]_{\mathrm{F}(\mathrm{x})=\mathrm{F}_{\mathrm{n}-1}(\mathrm{x})}\left(\mathrm{F}_{\mathrm{n}}(\mathrm{x})-\mathrm{F}_{\mathrm{n}-1}(\mathrm{n})\right)
$$

求解$\mathrm{L}\left(\mathrm{y}, \mathrm{F}_{\mathrm{n}}(\mathrm{x})\right)=0$；因此，$\left[\frac{\delta \mathrm{L}(\mathrm{y}, \mathrm{F}(\mathrm{x}))}{\delta \mathrm{F}(\mathrm{x})}\right]_{\mathrm{F}(\mathrm{x})=\mathrm{F}_{\mathrm{t}-1}(\mathrm{x})}$等价于损失函数为均方误差时的$y-F_{t-1}(x)$


💡 **优缺点**

1. 可处理**离散或连续**的数据，可以用于**分类或回归**；
2. **难以并行**，学习器之间的依赖性强；**对异常点敏感**

💡 **GBDT特征组合**

GBDT **采用最小化均方误差，来寻找分裂特征和分裂点**，会自动在当前特征分裂的基础上，继续寻找其他能使负梯度最小的特征，因此出现了特征组合的性能。


❓ **为什么不直接用残差？**

- 残差只是**损失函数负梯度的一种特例**，**而采用负梯度更通用**，可以使用不同损失函数。
- 残差就**等价于损失函数采用MSE时的负梯度**，而损失函数采用MSE其实会有一些缺点，比如对**异常值敏感**，所以在实际问题中我们有可能采用MAE或者更为折中的Huber损失函数

# XGBoost

[XGBoost、GBDT超详细推导](https://zhuanlan.zhihu.com/p/92837676)

**目标函数**

我们知道 XGBoost 是由K个基模型组成的一个加法运算式：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%207.png)

其中$f_k$为第 个基模型，$\hat{y}_i$为 第$i$个样本的预测值。

损失函数可由预测值$\hat{y}_i$与真实值${y}_i$进行表示：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%208.png)

我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，所以目标函数由模型的**损失函数$L$**与抑制模型复杂度的**正则项$\Omega$**组成：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%209.png)

**此时最优化目标函数，只有$f_t(x_i)$未知；**

这里正则化项进行了拆分，由于前 t-1棵树的结构已经确定，因此，前 t-1 棵树的复杂度之和可以用一个常量表示

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2010.png)

根据泰勒公式我们把函数$f(x+\Delta x)$ 在点$x$处进行泰勒的二阶展开，可得到如下等式：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2011.png)

我们把$\hat{y}_i^{t-1}$视为 $x$，$f_t(x_i)$视为$\Delta x$，故可以将目标函数写为：

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2012.png)

其中 $g_i$为损失函数的一阶导， $h_i$为损失函数的二阶导，注意这里的导是对$\hat{y}_i^{t-1}$求导。以平方损失函数为例$\sum_{i=1}^{n}\left(y_{i}-\left(\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)\right)^{2}$，有

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2013.png)

由于在第$t$步时$\hat{y}_i^{t-1}$其实是一个已知的值，所以$l(y_i,\hat{y}_i^{t-1})$是一个常数，其对函数的优化不会产生影响。所以只要求出损失函数的一阶和二阶导，然后最优化目标函数，就可以得到每一步的 
 $f(x)$，最后根据加法模型得到一个整体模型。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2014.png)

# 总结

💡  **分类树与回归树的区别**

1） 分类树使用信息增益或者增益比来划分节点；每个节点的类别经投票数额决定测试样本的类别；

2） 回归树使用最小化均方差划分节点；每个节点样本的均值作为测试样本的回归预测值


---

### **Random Forest**

💡 **随机森林
随机森林由很多决策树构成的，不同决策树之间没有关联。当新样本进入时，每棵决策树都会进行判断，然后基于投票法给出分类结果。
主要步骤：**

- **随机选择样本（放回抽样）**；
- **随机选择特征分裂，**直至不能分裂；
- 构建大量决策树形成随机森林；

💡 **优缺点：
优点：**

1. 可以处理**高维数据**，不用**特征选择**
2. 易于**并行化**，在大数据集上有很大的优势
3. 不容易**过拟合**

**缺点：**

1. 在**噪音较大**的数据集上会**过拟合**。
2. **取值划分较多的属性会对随机森林产生更大的影响，**所以产出的属性权值是不可信的。

💡 ****随机森林的随机性体现在哪里？
随机选取样本**训练决策树，**随机选择特征**构建决策树


💡 **为什么要有放回的抽样？**

**保证样本集间有重叠，若不放回，每个训练样本集及其分布都不一样，**可能导致各**决策树差异很大**，最终**投票表决**无法 “求同”。


💡 **随机森林如何处理缺失值？**

- 首先，给**缺失值设一些估计值**，比如中位数或众数。
- 然后，**根据估计值，建立随机森林**，。
- 测试所有样本，记录每个样本在树中的路径，**判断哪组样本和缺失样本路径最相似**。（缺失值是类别变量，通过投票得到新估计值；数值型变量，则加权平均）
- **迭代，**直到得到**稳定的估计值**。

💡 ****随机森林为什么不能用全样本取训练m棵决策树？****
随机森林的**基学习器是同构**，如果用全样本去训练m棵决策树的话，**基模型之间的多样性减少，相关性增加。**


💡 **随机森林需要剪枝吗？**

不需要，剪枝是为了**避免过拟合**，**随机森林随机选择样本与特征**，已经避免了过拟合。


💡 ****什么是OOB？随机森林中OOB是如何计算的?
OOB是袋外错误率oob error（out-of-bag error）；**随机森林**会随机且有放回的抽取样本**，每棵决策树会有**大概1/3的样本未抽取到**，这些样本就是每棵树的**OOB样本**。
这个**袋外数据**就可以用来**检测模型的泛化能力**


### Adaboost

💡 **Adaboost**

**AdaBoost主要思想：是前一个基分类器分错的样本会得到加强，加权后的样本再次被用来训练下一个基分类器，直到达到足够小的错误率或达到最大迭代次数。**

**Adaboost 迭代算法有三步：**

- **初始化训练样本的权值分布**，每个样本具有相同权重；
- 训练弱分类器，如果样本分类正确，它的权值就会被降低；反之提高。用更新过的数据集去训练下一个分类器；
- 将所有弱分类组合成强分类器，错误率低的分类器拥有更大权重

**损失函数：指数函数**

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2015.png)


💡 **Adaboost对噪声敏感吗?**
Adaboost会使**难分类样本的权值呈快速增长**，训练将会过于**偏向困难的样本**，导致Adaboost易受**噪声干扰**。


💡 **简述一下 Adaboost 的权值更新方法**

- 假设训练集样本总数 m。**样本初始权重**设置为$1/m$。
- 根据**样本权值分布**，训练出第一个弱分类器G1（对于无法接受带权样本的基学习算法，可以通过重采样处理）
- 根据基学习器的**分类误差**，规范化后重新给样本赋权重。

💡 **Adaboost为什么能快速收敛？**
因为每轮训练后，都会**增大上一轮错误的样本的权重**，下一轮的分类器**为了减少误差**，会**关注权重高的样本**，虽然每个分类器都可能分错，但权重大的样本更有可能正确，加速了收敛。


💡 **为什么Adaboost方式能够提高整体模型的学习精度?**
根据**前向分布算法**，Adabost算法每一次都会降低整体的误差，虽然单个模型误差会有波动，但是整体的误差却在降低。


💡 **Adaboost使用m个基学习器和加权平均使用m个学习器之间有什么不同?**
Adaboost的基学习器是有**顺序关系**的，每个基学习器在**不同分布的数据集上学习**，加权平均使用m个学习器更符合Adaboost的逻辑的。


💡 **Adaboost的迭代次数(基学习器的个数)如何控制?**
一般使用**early stopping**进行控制迭代次数。


💡 **Adaboost算法中基学习器是否很重要，应该怎么选择基学习器?**

sklearn中的adaboost接口给出的是使用决策树作为基分类器，一般认为决策树表现良好，其实可以根据数据的分布选择对应的分类器。


### GBDT

💡 **GBDT**

**GBDT模型是一个加法模型，它串行地训练一组CART回归树，最后对所有回归树的预测结果加和，每一颗新树都拟合当前损失函数的负梯度方向。**

- **损失函数**：
    - **回归：直接用连续的值计算负梯度**
        - 均方差
        - 绝对损失
        - huber损失：$L_{\delta}(y, f(x))=\left\{\begin{array}{ll}\frac{1}{2}(y-f(x))^{2} & \text { for }|y-f(x)| \leq \delta \\\delta|y-f(x)|-\frac{1}{2} \delta^{2} & \text { otherwise }\end{array}\right.$
    - **分类：指数（此时退化成为Adaboost）、对数**


💡 **优缺点：**

1. 可处理**离散或连续**的数据，可以用于**分类或回归**；
2. **难以并行**，学习器之间的依赖性强；**对异常点敏感**（使用一些健壮的损失函数，对异常值的鲁棒）

💡 **为什么GBDT中要拟合残差？**

GBDT拟合的**不是残差**，而是**负梯**度。只是当**损失函数为平方损失**的时候，负梯度正好为残差。


💡 **GBDT如何分类？**
本质上，GBDT的**回归和分类算法本质上没有区别**，但分类问题**输出是分类结果**，不能用来**拟合残差**。
解决方法：
损失函数使用**指数损失函数**，这时GBDT退化为Adaboost算法。
对数似然函数：$log(1 + exp(−yf(x)))$
**注：**多分类CrossEntropy作为损失函数（每次针对每个类都独立训练一个 CART树）


💡 **GBDT如何选择特征？**
GBDT的弱分类器默认选择的是CART树，GBDT选择特征其实就是CART树的生成过程；

- 首先，遍历所有特征，并且遍历所有特征的切分点
- 找到可以让损失函数最小的特征以及相应的切分点。

💡 **GBDT如何构建特征 ?**
利用gbdt去产生有效的特征组合，比如说GBDT生成了两个树，有五个叶子节点，假设输入样本$X$,它的输出落在了哪个叶子节点上，我们就将相应位置设为1，最终得到了一个5维向量作为该样本的组合特征，和原来的特征一起使用。


💡 **GBDT如何评估特征值的权重大小？**

1） 计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例作为权重值

2） 借鉴投票机制。用相同的GBDT参数对每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值


💡 **GBDT怎样设置单颗树的停止生长条件？**

1） 节点分裂时的最小样本数

2） 树的最大深度

3） 最多叶子节点数

4） Loss满足约束条件


💡 **GBDT中哪些部分可以并行**

1）计算每个样本的负梯度时

2）挑选最佳分裂特征及其分裂点时，计算特征相应的误差及均值时

3）更新每个样本的负梯度时

4）最后预测时，每个样本将之前的所有树的结果累加的时候


💡 **GBDT正则化方法**

- 训练时不采用全部数据进行训练，而是**按一定比例采样**数据
- 参考梯度下降学习率，给每个基学习器输出结果**乘上一个步长**， 即把更新公式变成$F_t​(x) = F_{t−1​}(x)+αf(x)$
- 对**基学习器进行正则化，剪枝**等操作

### XGBoost

💡 **XgBoost
XgBoost在GBDT的基础上，对损失函数进行了二阶泰勒展开，同时使用一阶二阶导数，二阶导数有利于梯度下降的更快更准。**

- **效率优化：**
    - **并行：**Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对分裂做了**并行优化，**XGBoost分别在**不同的线程中并行不同特征的划分**。
    - **内存优化：**对训练的每个特征排序并且**以块的的结构存储在内存中**，方便后面迭代重复使用。
- **效果优化：**
    - **缺失值：**XGBoost通过枚举所有缺失值划分左子树，还是右子树更优来处理缺失值
    - 引入了**适用于树模型的正则项**用于控制模型复杂度。
        - 包括叶子节点数、叶⼦节点权重的 L2 范式
        - XGBoost采用**预剪枝**策略，只有分裂后的增益大于0才会进行分裂

**缺点：**

- 虽然利⽤预排序可以降低寻找分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
- 预排序过程的空间复杂度⾼。

💡 **为什么XGBoost要用二阶泰勒展开，优势在哪里？**

- XGBoost使用了一阶和二阶偏导，**二阶导有利于梯度下降的更快更准**。一阶信息描述**梯度变化方向**，二阶信息可以描述**梯度变化方向是如何变化的**；可以类比**牛顿法**，牛顿法是也是二阶展开，当初始点选取合理时，牛顿法收敛的速度快。

![Untitled](%E5%86%B3%E7%AD%96%E6%A0%91%20df2ab3c2af41400e865ef49b8664100f/Untitled%2016.png)

- 并且**可以很方便地自定义损失函数**，只要这个损失函数可以求一阶和二阶导

💡 **XGBoost为什么可以并行训练？**

- XGBoost的并行，**并不是说每棵树可以并行训练**，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成。
- • XGBoost的并行，指的是**特征维度的并行**：在训练之前，**每个特征按特征值对样本进行预排序**，并存储为**Block结构**，在后面查找特征分割点时可以重复使用，并且可以利用多线程对每个block并行计算。

💡 ****XGBoost防止过拟合的方法？****

- 目标显式函数添加正则项
- **列抽样**：训练的时候只用一部分特征
- **子采样**：每轮计算可以不使用全部样本，使算法更加保守
- **shrinkage**: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间

💡 ****XGBoost如何处理缺失值？****

- XGBoost通过枚举所有缺失值划分左子树，还是右子树增益更大来处理缺失值
- 如果在训练中没有缺失值而预测中出现缺失，那么会自动将缺失值的划分到**右子结点**

💡 **如何停止树的生成?**

- 设置树的最大深度
- 损失函数的降低值的阈值
- 叶子节点样本太少，停止分裂（min_child_weight）

💡 ****5.XGBoost的参数调优有哪些经验？****
主要就是调****过拟合和欠拟合：****

一般**开始时是欠拟合**，调整树的数量n_estimator和深度max_depth，

**过拟合**主要是**调整损失函数**

- **树的复杂度**正则化参数gamma, 指定了节点分裂所需的最小的损失函数降低值
- alpha, L1正则化项的权重
- lambda，L2正则化项的权重

# 对比

💡 随机森林 **VS GBDT**

**相同：**

- 都是由**多棵树**组成的
- 预测结果都是**由多棵树一起决定**

**不同：**

- 随机森林采用的是**Bagging**的思想，而GBDT采用的是**Boosting**的思想；
- 随机森林的树可以是**分类树和回归树**，而GBDT只由**回归树**组成
- 随机森林的树可是**并行生成**，而GBDT只能是**串行生成**
- 随机森林对**异常值不敏感**，而GBDT对**异常值比较敏感**
- 对于最终的输出结果而言，随机森林采用**多数投票方法**，而GBDT则是将所有结果**加权累加**；

💡 **GBDT VS Adaboost**

**相同：**

- 都是 **Boosting 类算法**，使用弱分类器；
- **都是前向分布算法**（算法利用前一个学习器的结果来更新后一个学习器的训练集权重）。

**不同：**

- **迭代思路不同**：Adaboost 是通过**提升错分数据点的权重**来弥补模型的不足（**利用错分样本**），而 GBDT 是通过**拟合学习器的负梯度**来弥补模型的不足（**利用残差**）；
- **损失函数不同**：AdaBoost 采用的是**指数损失**，GBDT 使用的是**均方差或者绝对损失**；

💡 **GBDT VS XGBoost (重点)**

- GBDT将**目标函数泰勒展开到一阶**，而xgboost将**目标函数泰勒展开到了二阶**。保留了更多有关目标函数的信息，对提升效果有帮助。
- GBDT是给新的基模型**寻找新的拟合标签**（前面加法模型的负梯度），而xgboost是给新的基模型寻找新的**目标函数**（目标函数关于新的基模型的二阶泰勒展开）。
- XGBoost**显式加入了正则项，包括叶子节点数和叶子节点权重的L1、L2正则化项**，控制复杂度。
- XGBoost增加了**处理缺失值特征的策略**。把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣处理缺失值
- XGBoost支持**特征并行**等，提升性能。
- 传统GBDT使用CART作为**基分类器**；XGB支持**多种类型的基分类器**，如线性分类器

# 拓展

💡 ****Bagging 和 Boosting 优缺点及两者比较****

Bagging：Bagging中每个基学习器都会对训练集进行**有放回抽样**得到子训练集。每个基学习器基于子训练集独立训练，并综合所有基学习器的预测值得到最终的预测结果。

Boosting：Boosting 训练过程为**阶梯状**，基学习器的训练是有顺序的，**每个基学习器都会在前一个模型的基础上进行学习**，最终综合所有基学习器的预测得到最终的预测结果，用的比较多的综合方式为加权法。

- **比较**：
    1. **样本选择：**
        
        Bagging：训练集是有放回选取的，从原始集中选出的各轮训练集之间是独立的。
        Boosting：每一轮的训练集不变，每个样本在分类器中的权重发生变化。而权重是根据上一轮的分类结果调整
        
    2. **样本权重**
        
        Bagging：样本的权重相等
        
        Boosting：训练过程中不断调整样本的权值，提高错分样本权重
        
    3. **预测结果**
        
        Bagging：各个学习器的权重相等，通常使用投票法。
        
        Boosting：每个基学习器都有相应的权重，分类误差小的学习器有更大的权重，加权使用学习器。
        
    4. **并行**
        
        Bagging：各个学习器可以并行生成
        
        Boosting：各个学习器顺序生成，因为后一个学习器基于上一个学习器结果学习。
        

💡 **梯度下降 VS 梯度提升**

梯度下降是基于参数空间的，而梯度提升是基于函数空间；梯度提升算法是集成学习算法，每一个基学习器基于之前所有模型的预测结果学习，最终实现对模型的更新；梯度提升的思想本质就是梯度下降，


💡 **树生成畸形树，会带来哪些危害，如何预防？**

在生成树的过程中，加入树不平衡的约束条件。这种约束条件可以是用户自定义的。例如对样本集中分到某个节点，而另一个节点的样本很少的情况进行惩罚。


💡 **GBDT当增加样本数量时，训练时长是线性增加吗？**

不是的，因为生成单棵决策树时，对于损失函数极小值与样本数量N不是线性相关的


💡 **当增加树的棵数时，训练时长是线性增加吗？**

不是的，因为每棵树的生成时间复杂度O(N)不同


💡 **当增加一棵树叶子节点数目时，训练时长是线性增加吗？**

不是的，叶子节点数和每棵树的生成之间复杂度O(N)不成正比
