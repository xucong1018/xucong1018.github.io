# 梯度下降

[An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html#fn6)

[深度学习各类优化器详解（动量、NAG、adam、Adagrad、adadelta、RMSprop、adaMax、Nadam、AMSGrad）_恩泽君的博客-CSDN博客_动量优化器](https://blog.csdn.net/qq_42109740/article/details/105401197?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-105401197-blog-106426728.pc_relevant_downloadblacklistv1&spm=1001.2101.3001.4242.1&utm_relevant_index=2)

**梯度下降法的优化思想是用当前位置损失函数的负梯度方向作为搜索方向，因为该方向是损失函数最快下降方向，并且一般越接近目标值，步长越小，下降越慢。当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解。**

# ****Batch gradient descent (BGD)****

每次经过完整一轮训练后更新一次参数，这使得梯度下降过程变得比较慢，并且需要很大内存保存中间结果。

$$
θ=θ−η⋅∇θJ(θ)
$$

```python
for i in range(nb_epochs):
		params_grad = evaluate_gradient(loss_function, data, params)
		params =params - learning_rate * params_grad
```

💡 **需要计算整个数据集的梯度来执行一次更新，BGD下降可能会非常缓慢，批量梯度下降也不允许在线更新我们的模型。**


# ****Stochastic gradient descent (SGD）****

随机梯度下降是对每个训练样本就更新一次网络参数，这样使得网络**更新参数速度很快**，但是问题就是由于训练数据多样，**容易朝偏离网络最优点方向训练，网络训练不稳定**。
代码表示：

**for training example x(i) and label y(i)**

$$
θ=θ−η⋅∇θJ(θ;x(i);y(i))
$$

```python
for i in range(nb_epochs):
		np.random.shuffle(data)
		for example in data:
		params_grad= evaluate_gradient(loss_function, example, params)
		params =params - learning_rate * params_grad
```

💡


# ****Mini-batch gradient descent (MBGD)****

小批量梯度下降是批量梯度下降与随机梯度下降之间的一个折中，即经过一个小批量的训练数据更新一次参数，可以保证网络训练速度不太慢，也能使训练方向不至于偏离太多，具有一定稳定性。当使用小批量梯度下降时，通常也使用SGD这个术语。

$$
θ=θ−η⋅∇θJ(θ;x(i:i+n);y(i:i+n))
$$

代码表示：

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size=50):
        params_grad= evaluate_gradient(loss_function, batch, params)
        params =params - learning_rate * params_grad
```

💡 **减小了参数更新的方差，使收敛更加稳定;**


# ****梯度下降遇到的困难****

- **选择一个合适的学习率**是很困难的。学习率太小会导致收敛速度慢得令人痛苦，而学习率太大会阻碍收敛，导致损失函数在最小值附近波动，甚至发散。
- 上述问题可以通过提前定义一个学习速率表，当达到相应轮数或者阈值时根据表改变学习率，但是这样无法适应训练数据本身特征。
- 并且，对于所有参数我们使用同一个学习速率，**如果我们的数据是稀疏的或者我们特征具有不同的频率，我们可能不希望将它们更新到同样的程度**，并且我们希望对那些出现频率低的特征更新更快。
- 另外在神经网络中，普遍是具有非凸的误差函数，这使得在优化网络过程中，很容易陷入无数的局部最优点，而且更大困难往往也不是陷入局部最优点，而是来自鞍点（也就是在一个维度上其梯度是递增，另一个维度其梯度是递减，而在鞍点处其梯度为0），这些鞍点附近往往被相同误差点所包围，且在任意维度梯度近似为0，所以随机梯度下降很难从这些鞍点逃出。如下图：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled.png?raw=true)

# **动量优化法**

# ****Momentum****

随机梯度下降的方法很难通过峡谷区域（**也就是在一个维度梯度变化很大，另一个维度变化较小**），这个很好理解，因为梯度下降是梯度更新最大的反方向，如果这个时候一个维度梯度变化很大，那么就很容易在这个方向上振荡，另一个方向就更新很慢，如下图：

![SGD without momentum](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%201.png?raw=true)

SGD without momentum

![SGD with momentum](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%202.png?raw=true)

SGD with momentum

上面上图没有加动量，下图加了动量的方法，可以看到有动量可以在变化小的维度上加快更新，使得加快收敛。该方法是通过添加一个参数β构建一个一阶动量m,其中m有下列表达式：

💡 ****Momentum原理：通过将过去时间步的更新向量的一部分添加到当前更新向量。****


$$
v_t=γv_{t−1}+η∇_θJ(θ),\\ θ=θ−v_t
$$

本质上，当使用动量时，我们把一个球推下山。球在下坡的过程中积累了动量，速度越来越快(如果有空气阻力，它就会达到最终速度(γ<1 )。

# ****Nesterov accelerated gradient (NAG)****

动量的方法，我们发现参数更新是基于两部分组成，**一部分为当前位置的梯度，另一部分为前面累计下来的梯度值，参数更新方向就是将两者矢量相加的方向**，但是我们会发现一个问题，当刚好下降到山谷附近时，如果这个时候继续以这样的方式更新参数，我们会有一个较大的幅度越过山谷，即：模型遇到山谷不会自动减弱更新的幅度。NAG针对上述问题对动量方法进行了改进，其表达式如下，其中一阶动量项m如下，二阶动量为1。

$$
v_t=γv_{t−1}+η∇θJ(θ−γv_{t−1}),\\θ=θ−v_t
$$


💡 **预测后一步位置梯度**：它是利用当前位置处先前的梯度值先做一个参数更新，然后在更新后的位置再求梯度，将此部分梯度然后跟之前累积下来的梯度值矢量相加。有利于跳出局部最低点，但减慢了收敛速度。


# **自适应学习率优化算法**

# ****Adagrad****

假设我们用一批数据训练网络，这个数据中只有少部分数据含有某个特征，另一个特征几乎全部数据都具有，对于都含有的特征，这些神经元对应**参数更新很快**，但是对于那些只有**少部分数据含有的特征**，对应神经元权重获得**更新机会就少**，但是由于学习率一样，可能导致神经网络训练的不充分。

💡 让学习率学习数据的特征自动调整其大小，adagrad算法引入了二阶动量（**之前所有梯度平方和**）其表达式为：


![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%203.png?raw=true)

💡  Adagrad 的主要优势在于**不需要人为的调节学习率**，它可以自动调节；缺点在于，**随着迭代次数增多，学习率会越来越小，**最终会趋近于0。


# A****dadelta****

adadelta算法可以解决上述问题，其一阶向量跟adagrad一样，二阶参数有所变化（**之前所有梯度平方的移动平均**）

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%204.png?raw=true)

变形为：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%205.png?raw=true)

💡 不需要设置学习率


# ****RMSprop****

RMSprop算法由hinton教授提出，它与adadelta算法公式其实是一样的，他们是在相同时间被独立的提出，公式自然也为：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%206.png?raw=true)

hinton教授建议将**γ设置为0.9**，对于学习率，一个好的**固定值为0.001**。

# **Adam**

它是一种将动量和Adadelta或RMSprop结合起来的算法，也就引入了两个参数β1和β2，其一阶和二阶动量公式为：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/梯度下降/Untitled%207.png?raw=true)

💡 **Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要修改。**


![https://img-blog.csdnimg.cn/20200409013248963.gif#pic_center](https://img-blog.csdnimg.cn/20200409013248963.gif#pic_center)

![https://img-blog.csdnimg.cn/2020040901331652.gif#pic_center](https://img-blog.csdnimg.cn/2020040901331652.gif#pic_center)