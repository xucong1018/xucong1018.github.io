# 为什么标准化/归一化

[为什么要做特征的归一化/标准化？](https://mp.weixin.qq.com/s/SBYqggM5zrau6yLeeVkdTw)

**标准化使各个特征维度对⽬标函数的影响权重是⼀致的；原始特征下，因为尺度差异，模型的损失函数等高线图可能是椭圆形。**

**梯度方向垂直于等高线，下降会走zigzag路线，而不是指向local minimum。**

![Untitled](%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A0%87%E5%87%86%E5%8C%96%20%E5%BD%92%E4%B8%80%E5%8C%96%2040524e985ecc4c398af238331746f026/Untitled.png)

## 归一化和标准化的区别：

- 归一化：数据线性映射到0-1之间，**无量纲化**（可以忽略单位对计算的影响），使不同量纲的数据具有可比性
- 标准化：常用的方法是**z-score标准化**，经过处理后的数据均值为0，标准差为1

从公式可知，标准化和归一化两者都是对样本进行平移缩放

## 常用方法

- **Rescaling (min-max normalization、range scaling)：**
    
    将每一维特征线性映射到目标范围[a,b]，即将最小值映射为a，最大值映射为b，常用目标范围为[0,1]和[−1,1]，特别地，映射到[0,1]计算方式为：
    

[https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2rkJlnTEgpwR2ej1uTBmMniawdIT5tFq9AIeWdJTR1ULDcTpy4dWKL5g/640?wx_fmt=png&random=0.24063842088854037&random=0.06423164059919051&random=0.39807915990488363&random=0.616354439638384&random=0.9187608790400732&random=0.9234645215000226&random=0.4516069917463543&wxfrom=5&wx_lazy=1&wx_co=1](https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2rkJlnTEgpwR2ej1uTBmMniawdIT5tFq9AIeWdJTR1ULDcTpy4dWKL5g/640?wx_fmt=png&random=0.24063842088854037&random=0.06423164059919051&random=0.39807915990488363&random=0.616354439638384&random=0.9187608790400732&random=0.9234645215000226&random=0.4516069917463543&wxfrom=5&wx_lazy=1&wx_co=1)

- **Mean normalization：**
    
    将均值映射为0，同时用最大值最小值的差对特征进行归一化，一种更常见的做法是用标准差进行归一化，如下。
    

[https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2LyEAhWW02D3M7ZrfQcugho5QhNqv47icfTVRicTs8cEI1dcq0MUvs17g/640?wx_fmt=png&random=0.1842320918103557&random=0.2658764103376916&random=0.21219714055175554&random=0.733935352595593&random=0.6358706523054567&random=0.4123771707583852&random=0.08771318372005288&wxfrom=5&wx_lazy=1&wx_co=1](https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2LyEAhWW02D3M7ZrfQcugho5QhNqv47icfTVRicTs8cEI1dcq0MUvs17g/640?wx_fmt=png&random=0.1842320918103557&random=0.2658764103376916&random=0.21219714055175554&random=0.733935352595593&random=0.6358706523054567&random=0.4123771707583852&random=0.08771318372005288&wxfrom=5&wx_lazy=1&wx_co=1)

- **Standardization (Z-score Normalization)**：
    
    每维特征**0均值1方差（zero-mean and unit-variance）**。
    

[https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2qyqj3W3qRBMwxpyQ9DTh17gaZdSq0Lx7OWeWO9KywpXscmuJGPAibZw/640?wx_fmt=png&random=0.7896024264053698&random=0.6638980219481698&random=0.10141251859427847&random=0.0730639058719591&random=0.9932525984034883&random=0.7720234723677115&random=0.9054979195693231&wxfrom=5&wx_lazy=1&wx_co=1](https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2qyqj3W3qRBMwxpyQ9DTh17gaZdSq0Lx7OWeWO9KywpXscmuJGPAibZw/640?wx_fmt=png&random=0.7896024264053698&random=0.6638980219481698&random=0.10141251859427847&random=0.0730639058719591&random=0.9932525984034883&random=0.7720234723677115&random=0.9054979195693231&wxfrom=5&wx_lazy=1&wx_co=1)

- **Scaling to unit length**：
    
    将每个样本的特征向量除以其长度，即对样本特征向量的长度进行归一化，长度的度量常使用的是L2 norm（欧氏距离），有时也会采用L1 norm。
    

[https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2FvgsQMc88nMbCNbu1fcT6JaGaibRrbYLcIibVJdjlWYYib1q5NSnAlUaQ/640?wx_fmt=png&random=0.5827931688312142&random=0.5288670559073372&random=0.49151166062407103&random=0.4932580995682805&random=0.8050215926931332&random=0.3504147480821982&random=0.5120308933235631&wxfrom=5&wx_lazy=1&wx_co=1](https://mmbiz.qpic.cn/mmbiz_png/jupejmznDC8EFOZCndd2SXChnLGan8b2FvgsQMc88nMbCNbu1fcT6JaGaibRrbYLcIibVJdjlWYYib1q5NSnAlUaQ/640?wx_fmt=png&random=0.5827931688312142&random=0.5288670559073372&random=0.49151166062407103&random=0.4932580995682805&random=0.8050215926931332&random=0.3504147480821982&random=0.5120308933235631&wxfrom=5&wx_lazy=1&wx_co=1)

<aside>
💡 **拓展**

- 前3种feature scaling的计算方式为**减一个统计量再除以一个统计量**，最后1种为**除以向量自身的长度**。
- **减一个统计量**可以看成**选哪个值作为原点，是最小值还是均值，并将整个数据集平移到这个新的原点位置**。如果特征间偏置不同对后续过程有负面影响，则该操作是有益的，可以看成是某种**偏置无关操作**；如果原始特征值有特殊意义，比如稀疏性，该操作可能会破坏其稀疏性。
- **除以一个统计量**可以看成在**坐标轴方向上对特征进行缩放**，用于**降低特征尺度的影响，可以看成是某种尺度无关操作**。缩放可以使用最大值最小值间的跨度，也可以使用标准差（到中心点的平均距离），前者对outliers敏感，outliers对后者影响与outliers数量和数据集大小有关，outliers越少数据集越大影响越小。
- **除以长度**相当于把长度归一化，**把所有样本映射到单位球上**，可以看成是某种**长度无关操作**，比如，词频特征要移除文章长度的影响，图像处理中某些特征要移除光照强度的影响，以及方便计算余弦距离或内积相似度等。
</aside>

## ****什么时候需要feature scaling？****

- **涉及或隐含距离计算的算法**；比如K-means、KNN、PCA、SVM等
    
    **zero-mean一般可以增加样本间余弦距离或者内积结果的差异**，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。
    
- **损失函数中含有正则项时**
    
    对于线性模型$y=wx+b$而言，x的任何线性变换（平移、放缩），都可以被w和b吸收，理论上，不会影响模型的拟合能力。**但是，如果损失函数中含有正则项，如$λ∣∣w∣∣^2$，λ为超参数，其对w的每一个参数施加同样的惩罚**，但对于某一维特征$X_i$而言，其scale越大，系数$w_i$越小，其在正则项中的比重就会变小，惩罚变小，即损失函数会相对忽视那些scale增大的特征，所以需要feature scaling，使损失函数平等看待每一维特征。
    
- **梯度下降算法**
    
    ![Untitled](%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A0%87%E5%87%86%E5%8C%96%20%E5%BD%92%E4%B8%80%E5%8C%96%2040524e985ecc4c398af238331746f026/Untitled%201.png)
    
    E(W)为损失函数，**收敛速度取决于：参数的初始位置到local minima的距离，以及学习率η大小**。一维情况下，**在local minima附近**，不同学习率对梯度下降的影响如下图所示：
    
    ![Untitled](%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A0%87%E5%87%86%E5%8C%96%20%E5%BD%92%E4%B8%80%E5%8C%96%2040524e985ecc4c398af238331746f026/Untitled%202.png)
    
    - 多维情况下可以分解成多个上图，每个维度上分别下降，参数W为向量，但学习率只有1个，**即所有参数维度共用同一个学习率**（暂不考虑为每个维度都分配单独学习率的算法）。收敛意味着在每个参数维度上都取得极小值，每个参数维度上的偏导数都为0，但是每个参数维度上的下降速度是不同的，为了每个维度上都能收敛，学习率应取所有维度在当前位置合适步长中最小的。
    - 对于传统的神经网络，采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，导致**梯度消失**，需要对输入做Standardization或映射到[0,1]、[−1,1]对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，可以不需要对网络的输入进行feature scaling了，但习惯上还是会做feature scaling。****

## **什么时候不需要Feature Scaling？**

- **与距离计算无关的概率模型；比如Naive Bayes；**
- **与距离计算无关的基于树的模型；**比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。