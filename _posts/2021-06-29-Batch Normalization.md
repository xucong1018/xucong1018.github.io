---
layout:     post
title:      Batch Normalization
subtitle:   Batch Normalization
date:       2021-03-29
author:     BY
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - Normalization
---

# Batch Normalization

[Layer Normalization](https://zhuanlan.zhihu.com/p/338778158)

[#深入理解# BN LN IN GN几种标准化的区别](https://zhuanlan.zhihu.com/p/354837787)

## BN&LN****区别****

**BN的主要思想是：针对一个batch里面的数据进行规范化，在一个batch里的同一个特征通道上进行标准化。**

**LN的主要思想是：是对每一个样本的所有特征做归一化，而不是 BN 那种在批方向计算均值和方差。**

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/BatchNormalization/Untitled.png?raw=true)

# BN

## **BN的作用**

一、**加快网络的训练和收敛的速度**

如果每层的**数据分布都不一样**的话，将会导致网络非常**难收敛和训练**，而如果把每层的数据都在转换在均值为零，方差为1 的状态下，这样每层数据的分布都是一样的训练会比较容易收敛。

二、**控制梯度爆炸防止梯度消失**

- **梯度消失：**以sigmoid函数为例，sigmoid函数使得输出在[0,1]之间，实际上当**x过大或者过小**，经过sigmoid函数后**输出范围就会变得很小**，而且反向传播时的**梯度也会非常小**，从而导致**梯度消失**，同时也会导致网络学习速率过慢；

Batch Normalization (BN) 通常被添加在每一个全连接和激励函数之间，使数据在进入激活函数之前**集中分布在0值附近**，大部分激活函数输入在0周围时输出会有加大变化，下图能够很好的解释这个问题：

![Untitled](https://github.com/xucong1018/xucong1018.github.io/blob/master/img/BatchNormalization/Untitled%201.png?raw=true)

- **梯度爆炸：**在方向传播的过程中，每一层的梯度都是由上一层的梯度乘以本层的数据得到。如果标准化的话，**数据均值都在0附近**，很显然，每一层的梯度不会产生爆炸的情况。

三、**防止过拟合**

在网络的训练中，BN的使用**使得一个minibatch中所有样本都被关联**在了一起，因此网络不会从某一个训练样本中生成确定的结果，**即同样一个样本的输出不再仅仅取决于样本的本身**，也取决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝这一个方向使劲学习。一定程度上避免了过拟合。

## **BN在测试和训练时上有何不同？**

- 训练时，BN层每次都会用每个Batch内数据的均值和标准差对Batch内的样本进行标准化；同时会通过滑动平均的方式不断更新训练集上全局均值和方差并保存
- 测试时，用**保存好的训练集上的均值和方差**对测试集上的每个数据进行标准化

❓ **BN训练时为什么不⽤全量训练集的均值和⽅差呢？**

因为在训练的第⼀个完整 epoch过程中是⽆法得到输⼊层之外其他层全量训练集的均值和⽅差，只能在前向传播过程中获取已训练 batch的均值和⽅差。


❓ **那在⼀个完整 epoch之后可以使⽤全量数据集的均值和⽅差嘛？**

对于 BN，是对**每⼀批数据进⾏标准化到⼀个相同的分布**，⽽且每⼀批数据的均值和⽅差会有⼀定的差别，⽽不是⽤**固定的值**，这个差别实际上也能够增加**模型的鲁棒性**，也会在⼀定程度上减少过拟合。但是⼀批数据和全量数据的均值和⽅差相差太多，⼜⽆法较好地代表训练集的分布，因此， BN⼀般要求将训练集完全打乱，并⽤⼀个较⼤的 batch值，去缩⼩与全量数据的差别。


# LN

**Layer Normalization 是对一个样本中的所有数据进行标准化**；因为RNN一般用来处理序列数据，大部分序列类型的数据样本长度往往存在较大差异，**如果使用BN处理就会出现某些通道没有某些样本数据的问题，即使使用特定字符填充，也会造成某些通道分布不均匀的问题**

💡 **BN、LN的差别和应用场景**

- BN在特征通道方向上做标准化，BN对CNN效果很好，因为CNN本身的目的就是结合不同batch的特征，做特征提取；
- LN对句子序列中的一个一个的词做标准化，对时序特征的效果特别好；
- IN是对整体单个输入（一张图）做标准化，所以在风格迁移的时候，能对单一风格做到非常好的特征提取。

💡 **缺陷**

- BN丢失了**通道之间的数据联系**，因为是对每个通道进行的标准化，因此通道之间的关联自然就丢失了,
- **LN对于一整层的神经元训练得到同一个转换**——所有的输入都在同一个区间范围内。如果**不同输入特征不属于相似的类别**（比如颜色和大小)，那么LN的处理可能会降低模型的表达能力。
